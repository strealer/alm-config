# Vector configuration for Strealer ALM edge devices
# Optimized for Raspberry Pi 4 (Resource Constrained)
api:
  enabled: true
  address: "0.0.0.0:8686"
# ============================================================================
# SOURCES
# ============================================================================
sources:
  nginx_access:
    type: file
    include: ["/var/log/nginx/access*.log"]
    read_from: end
    max_line_bytes: 102400
  nginx_error:
    type: file
    include: ["/var/log/nginx/error*.log"]
    read_from: end
    max_line_bytes: 102400
  docker_containers:
    type: docker_logs
    docker_host: "unix:///var/run/docker.sock"
    include_containers: ["alm_arm64", "alm_telemetry", "alm_init"]
    exclude_containers: []
    partial_event_marker_field: "_partial"
    auto_partial_merge: true
  system_logs:
    type: journald
    include_units: ["docker", "systemd"]
  # Internal Vector metrics for self-monitoring
  vector_metrics:
    type: internal_metrics
    namespace: vector
  # Host system metrics (CPU, Memory, Disk, Network)
  host_metrics:
    type: host_metrics
    collectors:
      - cpu # CPU usage per core and total
      - memory # Memory usage (used, available, free, cached)
      - disk # Disk I/O statistics
      - filesystem # Filesystem usage per mount point
      - network # Network I/O per interface
      - load # System load averages (1m, 5m, 15m)
    scrape_interval_secs: 15
    namespace: host
  # Collect IP information (Local and Public)
  ip_info:
    type: "exec"
    command: ["/bin/sh", "-c", "echo \"{\\\"public_ip\\\": \\\"$(wget -qO- checkip.amazonaws.com 2>/dev/null)\\\", \\\"local_ip\\\": \\\"$(ip route get 1.1.1.1 2>/dev/null | grep -oE 'src [0-9.]+' | cut -d' ' -f2)\\\"}\""]
    mode: "scheduled"
    scheduled:
      exec_interval_secs: 300 # Run every 5 minutes (lightweight command, minimal overhead)
    decoding:
      codec: "json"
  # Collect CPU temperature from thermal zone
  cpu_temperature:
    type: "exec"
    command: ["/bin/sh", "-c", "cat /sys/class/thermal/thermal_zone0/temp 2>/dev/null || echo '0'"]
    mode: "scheduled"
    scheduled:
      exec_interval_secs: 15 # Match host_metrics scrape interval
    decoding:
      codec: "bytes"
# ============================================================================
# TRANSFORMS
# ============================================================================
transforms:
  # Optimize Nginx parsing for CPU efficiency
  parse_nginx_access:
    type: remap
    inputs: ["nginx_access"]
    drop_on_error: true
    reroute_dropped: true
    source: "message = string!(.message)\n.source_type = \"nginx_access\"\n\n# Parse nginx custom log format\n# Format: \"FT | $time_iso8601 | $status | $request | $upstream_cache_status | $body_bytes_sent\"\nparsed, err = parse_regex(message, r'^(?P<log_type>[A-Z]+) \\| (?P<timestamp>[^ ]+) \\| (?P<status>\\d+) \\| (?P<request>[^|]+) \\| (?P<cache_status>[^|]+) \\| (?P<bytes>\\d+)$')\n\nif err == null {\n  .log_type = parsed.log_type\n  .timestamp = parse_timestamp!(parsed.timestamp, \"%Y-%m-%dT%H:%M:%S%z\")\n  .status = to_int!(parsed.status)\n  .request = parsed.request\n  .cache_status = parsed.cache_status\n  .bytes_sent = to_int!(parsed.bytes)\n  \n  # Extract request method and path\n  request_parts = split!(parsed.request, \" \", limit: 3)\n  .method = request_parts[0]\n  .path = request_parts[1]\n  .http_version = request_parts[2]\n  \n  # Categorize by cache type\n  if contains(string!(.path), \"/media\") {\n    .cache_type = \"media\"\n  } else if contains(string!(.path), \"/player\") {\n    .cache_type = \"player\"\n  } else if contains(string!(.path), \"/renderer\") {\n    .cache_type = \"renderer\"\n  } else {\n    .cache_type = \"other\"\n  }\n}\n"
  # Sampling for high-volume logs (keep every 10th success)
  sample_nginx_access:
    type: sample
    inputs: ["parse_nginx_access"]
    rate: 10
  # Add device metadata (using Lua for consistency and file reading)
  add_metadata:
    type: lua
    inputs: ["sample_nginx_access", "nginx_error", "docker_containers", "system_logs", "ip_info"]
    version: "2"
    hooks:
      process: "function (event, emit)\n  -- Lazy load hostname from file\n  if not _G.device_hostname then\n    local f = io.open(\"/etc/hostname\", \"r\")\n    if f then\n      local content = f:read(\"*all\")\n      f:close()\n      content = content:gsub(\"%s+\", \"\")\n      local parts = {}\n      for part in string.gmatch(content, \"[^-]+\") do\n        table.insert(parts, part)\n      end\n      if #parts >= 2 then\n        _G.device_hostname = parts[2]\n      else\n        _G.device_hostname = content\n      end\n    else\n      _G.device_hostname = \"unknown\"\n    end\n  end\n\n  if _G.device_hostname then\n    event.log.device_hostname = _G.device_hostname\n  end\n  event.log.device_role = \"alm-edge\"\n  event.log.environment = \"production\"\n  if not event.log.cache_status then\n    event.log.cache_status = \"N/A\"\n  end\n  \n  -- Ensure IP fields are available for labels\n  -- Check if fields exist (from codec), otherwise parse from message\n  if not event.log.public_ip or not event.log.local_ip then\n     local message = event.log.message\n     if message then\n       local public_ip = string.match(message, '\"public_ip\":%s*\"([^\"]+)\"')\n       local local_ip = string.match(message, '\"local_ip\":%s*\"([^\"]+)\"')\n       \n       if public_ip and not event.log.public_ip then\n         event.log.public_ip = public_ip\n       end\n       if local_ip and not event.log.local_ip then\n         event.log.local_ip = local_ip\n       end\n     end\n  end\n  \n  -- Default to empty string if still missing to prevent label errors\n  if not event.log.public_ip then event.log.public_ip = \"\" end\n  if not event.log.local_ip then event.log.local_ip = \"\" end\n  \n  \n  emit(event)\nend\n"
  # Enrich host metrics with correct hostname from mounted file (using Lua since VRL can't read files)
  enrich_host_metrics:
    type: lua
    inputs: ["host_metrics"]
    version: "2"
    hooks:
      process: |
        function (event, emit)
          -- Lazy load hostname from file
          if not _G.device_hostname then
            local f = io.open("/etc/hostname", "r")
            if f then
              local content = f:read("*all")
              f:close()
              content = content:gsub("%s+", "")
              local parts = {}
              for part in string.gmatch(content, "[^-]+") do
                table.insert(parts, part)
              end
              if #parts >= 2 then
                _G.device_hostname = parts[2]
              else
                _G.device_hostname = content
              end
            else
              _G.device_hostname = "unknown"
            end
          end

          if _G.device_hostname then
            event.metric.tags.host = _G.device_hostname
          end
          -- Debug print to verify metrics are passing through
          -- print("Processing metric: " .. (event.metric.name or "unknown"))
          emit(event)
        end
  # Convert temperature from millidegrees to degrees Celsius
  convert_temperature:
    type: remap
    inputs: ["cpu_temperature"]
    source: |
      # Convert from millidegrees to degrees (divide by 1000)
      temp_millidegrees = to_int!(.message)
      .temperature_celsius = temp_millidegrees / 1000.0
  # Convert CPU temperature to metric with hostname
  temperature_to_metric:
    type: lua
    inputs: ["convert_temperature"]
    version: "2"
    hooks:
      process: |
        function (event, emit)
          -- Lazy load hostname
          if not _G.device_hostname then
            local f = io.open("/etc/hostname", "r")
            if f then
              local content = f:read("*all")
              f:close()
              content = content:gsub("%s+", "")
              local parts = {}
              for part in string.gmatch(content, "[^-]+") do
                table.insert(parts, part)
              end
              if #parts >= 2 then
                _G.device_hostname = parts[2]
              else
                _G.device_hostname = content
              end
            else
              _G.device_hostname = "unknown"
            end
          end

          -- Create new metric event (metric events must have metric key at top level)
          emit {
            metric = {
              kind = "absolute",
              name = "cpu_temperature_celsius",
              namespace = "host",
              timestamp = event.log.timestamp,
              tags = {
                host = _G.device_hostname,
                sensor = "thermal_zone0"
              },
              gauge = {
                value = tonumber(event.log.temperature_celsius) or 0
              }
            }
          }
        end
  # Filter only parsed nginx logs for metrics (logs with status field)
  nginx_parsed_logs:
    type: filter
    inputs: ["add_metadata"]
    condition:
      type: vrl
      source: exists(.status) && exists(.method)
  # Generate metrics from parsed nginx logs (for Prometheus)
  log_metrics:
    type: log_to_metric
    inputs: ["nginx_parsed_logs"]
    metrics:
      - type: counter
        field: status
        name: http_requests_total
        namespace: strealer
        tags:
          status: "{{ status }}"
          method: "{{ method }}"
          cache_status: "{{ cache_status }}"
          device: "{{ device_hostname }}"
# ============================================================================
# SINKS
# ============================================================================
sinks:
  loki:
    type: loki
    inputs: ["add_metadata"]
    endpoint: "${LOKI_ENDPOINT:-http://your-loki-server:3100}"
    encoding:
      codec: json
    healthcheck:
      enabled: true
    compression: snappy
    # Reduced label cardinality for Loki performance
    labels:
      device: "{{ device_hostname }}"
      role: "alm-edge"
      env: "production"
      log_source: "{{ source_type }}"
      cache_status: "{{ cache_status }}"
      public_ip: "{{ public_ip }}"
      local_ip: "{{ local_ip }}"
    # Disk buffering for edge resilience
    buffer:
      type: disk
      max_size: 283115520 # 270MB (increased from 256MB to satisfy min requirement)
      when_full: block
    # Optimized batching for edge
    batch:
      max_bytes: 262144 # 256KB
      timeout_secs: 15
      max_events: 1000
    # Request limits
    request:
      timeout_secs: 30
      rate_limit_num: 100
      rate_limit_duration_secs: 1
      retry_attempts: 5
    # Data durability
    acknowledgements:
      enabled: true
  # Metrics: Push to Prometheus (Remote Write)
  prometheus_remote_write:
    type: prometheus_remote_write
    inputs: ["log_metrics", "vector_metrics", "enrich_host_metrics", "temperature_to_metric"] # Send generated metrics + internal + enriched host metrics + temperature
    endpoint: "${PROMETHEUS_ENDPOINT:-http://your-prometheus:9090}/api/v1/write"
    default_namespace: "strealer"
    healthcheck:
      enabled: false  # Healthcheck uses GET which is not allowed on /api/v1/write (POST only)
  # Expose metrics locally for scraping (optional backup)
  prometheus_exporter:
    type: prometheus_exporter
    inputs: ["log_metrics", "vector_metrics", "enrich_host_metrics", "temperature_to_metric"]
    address: "0.0.0.0:9598"
    default_namespace: "strealer"
