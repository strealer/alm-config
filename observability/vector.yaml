# Vector configuration for Strealer ALM edge devices
# Optimized for Raspberry Pi 4 (Resource Constrained)
api:
  enabled: true
  address: "0.0.0.0:8686"
data_dir: "/var/lib/vector"
# ============================================================================
# SOURCES
# ============================================================================
sources:
  nginx_access:
    type: file
    include: ["/var/log/alm/nginx/access*.log"]
    read_from: end
    max_line_bytes: 102400
  nginx_error:
    type: file
    include: ["/var/log/alm/nginx/error*.log"]
    read_from: end
    max_line_bytes: 102400
  docker_containers:
    type: docker_logs
    docker_host: "unix:///var/run/docker.sock"
    include_containers: ["alm_arm64", "alm_telemetry", "alm_init"]
    exclude_containers: []
    partial_event_marker_field: "_partial"
    auto_partial_merge: true
  # OPTIMIZED: Filter by priority to reduce volume by 90%
  # Only collect WARNING (4) and above (ERROR=3, CRITICAL=2, ALERT=1, EMERGENCY=0)
  # This filters out INFO (6) and DEBUG (7) which are 90% of volume
  # FIXED: Removed include_units to prevent OR bypass (units now only in include_matches)
  system_logs:
    type: journald
    include_matches:
      _SYSTEMD_UNIT:
        - docker.service
        - puppet.service
        - vector.service
      PRIORITY:
        - "0"
        - "1"
        - "2"
        - "3"
        - "4"
    batch_size: 64 # Increased from default 16 for better throughput
  # Internal Vector metrics for self-monitoring
  vector_metrics:
    type: internal_metrics
    namespace: vector
  # Host system metrics (CPU, Memory, Disk, Network)
  # OPTIMIZED: Reduced from 15s to 30s (50% less frequent, still adequate granularity)
  host_metrics:
    type: host_metrics
    collectors:
      - cpu # CPU usage per core and total
      - memory # Memory usage (used, available, free, cached)
      - disk # Disk I/O statistics
      - filesystem # Filesystem usage per mount point
      - network # Network I/O per interface
      - load # System load averages (1m, 5m, 15m)
    scrape_interval_secs: 30
    namespace: host
  # Collect IP information (Local and Public)
  ip_info:
    type: "exec"
    command: ["/bin/sh", "-c", "echo \"{\\\"public_ip\\\": \\\"$(wget -qO- checkip.amazonaws.com 2>/dev/null)\\\", \\\"local_ip\\\": \\\"$(ip route get 1.1.1.1 2>/dev/null | grep -oE 'src [0-9.]+' | cut -d' ' -f2)\\\"}\""]
    mode: "scheduled"
    scheduled:
      exec_interval_secs: 300 # Run every 5 minutes (lightweight command, minimal overhead)
    decoding:
      codec: "json"
  # Collect CPU temperature from thermal zone
  # OPTIMIZED: Reduced from 15s to 30s to match host_metrics interval
  cpu_temperature:
    type: "exec"
    command: ["/bin/sh", "-c", "cat /sys/class/thermal/thermal_zone0/temp 2>/dev/null || echo '0'"]
    mode: "scheduled"
    scheduled:
      exec_interval_secs: 30 # Match host_metrics scrape interval
    decoding:
      codec: "bytes"
  # NEW: Collect kernel messages (hardware issues, OOM, thermal throttling)
  kernel_logs:
    type: journald
    include_matches:
      _TRANSPORT:
        - kernel
      PRIORITY:
        - "0"
        - "1"
        - "2"
        - "3"
        - "4"
    batch_size: 64
  # NEW: Collect per-container CPU and memory usage
  docker_stats:
    type: "exec"
    command: ["/bin/sh", "-c", "docker stats --no-stream --format '{{json .}}' --all | jq -c '{name:.Name,cpu:.CPUPerc,mem:.MemPerc,net:.NetIO,block:.BlockIO}'"]
    mode: "scheduled"
    scheduled:
      exec_interval_secs: 30 # Match host_metrics interval
    decoding:
      codec: "json"
  # NEW: Monitor critical service health
  service_health:
    type: "exec"
    command: ["/bin/sh", "-c", "echo '{\"docker\":\"'$(systemctl is-active docker)'\",\"puppet\":\"'$(systemctl is-active puppet)'\",\"vector\":\"'$(systemctl is-active vector)'\",\"strealer\":\"'$(systemctl is-active strealer-container)'\"}'"]
    mode: "scheduled"
    scheduled:
      exec_interval_secs: 60 # Check every minute
    decoding:
      codec: "json"
  # NEW: Collect top 10 processes by CPU usage with detailed metrics
  # Output: NDJSON format (one JSON object per line, not an array)
  top_processes:
    type: "exec"
    command: ["/bin/sh", "-c", "ps -eo pid,ppid,user,comm,state,%cpu,%mem,rss,vsz --sort=-%cpu | head -11 | awk 'NR==1 {next} {print \"{\\\"pid\\\":\\\"\" $$1 \"\\\",\\\"ppid\\\":\\\"\" $$2 \"\\\",\\\"user\\\":\\\"\" $$3 \"\\\",\\\"command\\\":\\\"\" $$4 \"\\\",\\\"state\\\":\\\"\" $$5 \"\\\",\\\"cpu\\\":\" $$6 \",\\\"mem\\\":\" $$7 \",\\\"rss_kb\\\":\" $$8 \",\\\"vsz_kb\\\":\" $$9 \"}\"}'"]
    mode: "scheduled"
    scheduled:
      exec_interval_secs: 60 # Collect every minute
    framing:
      method: "newline_delimited"
    decoding:
      codec: "json"
  # NEW: Collect Docker image metadata (Digest, Created, Size)
  # State time shows: for running = empty (Created At is sufficient), for stopped = exit timestamp
  docker_images_info:
    type: "exec"
    command: ["/bin/sh", "-c", "docker ps -a --format '{{.Names}}' | grep alm | while read c; do img=$$(docker inspect -f '{{.Image}}' $$c 2>/dev/null); if [ -n \"$$img\" ]; then state=$$(docker inspect -f '{{.State.Status}}' $$c); finished=$$(docker inspect -f '{{.State.FinishedAt}}' $$c); if [ \"$$state\" = \"running\" ]; then state_time=\"\"; else state_time=\"$$finished\"; fi; printf '{\"img_container\":\"%s\",\"img_repository\":\"%s\",\"img_digest\":\"%s\",\"img_created\":\"%s\",\"img_size_mb\":\"%.2f\",\"img_state\":\"%s\",\"img_state_time\":\"%s\"}\\n' \"$$c\" \"$$(docker inspect -f '{{if .RepoTags}}{{index .RepoTags 0}}{{else}}{{if .RepoDigests}}{{index .RepoDigests 0}}{{else}}none{{end}}{{end}}' $$img)\" \"$$(docker inspect -f '{{if .RepoDigests}}{{index .RepoDigests 0}}{{else}}none{{end}}' $$img)\" \"$$(docker inspect -f '{{.Created}}' $$img)\" \"$$(echo $$(docker inspect -f '{{.Size}}' $$img) | awk '{print $$1/1048576}')\" \"$$state\" \"$$state_time\"; fi; done"]
    mode: "scheduled"
    scheduled:
      exec_interval_secs: 300 # Collect every 5 minutes
    framing:
      method: "newline_delimited"
    decoding:
      codec: "json"
  # PSI (Pressure Stall Information) - High-resolution resource pressure monitoring
  # Captures ALL CPU/Memory/IO stalls via cumulative 'total' counters (no spike is ever missed)
  # Ref: https://docs.kernel.org/accounting/psi.html
  psi_pressure:
    type: "exec"
    command: ["/bin/sh", "-c", "if [ -f /proc/pressure/cpu ]; then cpu_some=$$(awk '/^some/{print $$NF}' /proc/pressure/cpu | cut -d= -f2); cpu_full=$$(awk '/^full/{print $$NF}' /proc/pressure/cpu | cut -d= -f2); mem_some=$$(awk '/^some/{print $$NF}' /proc/pressure/memory | cut -d= -f2); mem_full=$$(awk '/^full/{print $$NF}' /proc/pressure/memory | cut -d= -f2); io_some=$$(awk '/^some/{print $$NF}' /proc/pressure/io | cut -d= -f2); io_full=$$(awk '/^full/{print $$NF}' /proc/pressure/io | cut -d= -f2); printf '{\"psi_cpu_some\":%s,\"psi_cpu_full\":%s,\"psi_mem_some\":%s,\"psi_mem_full\":%s,\"psi_io_some\":%s,\"psi_io_full\":%s}\\n' \"$$cpu_some\" \"$$cpu_full\" \"$$mem_some\" \"$$mem_full\" \"$$io_some\" \"$$io_full\"; fi"]
    mode: "scheduled"
    scheduled:
      exec_interval_secs: 1 # High-resolution: 1 second polling
    framing:
      method: "newline_delimited"
    decoding:
      codec: "json"
# ============================================================================
# TRANSFORMS
# ============================================================================
transforms:
  # Optimize Nginx parsing for CPU efficiency
  parse_nginx_access:
    type: remap
    inputs: ["nginx_access"]
    drop_on_error: true
    reroute_dropped: false
    source: "message = string!(.message)\n.source_type = \"nginx_access\"\n\n# Parse nginx custom log format\nparsed, err = parse_regex(message, r'^(?P<log_type>[A-Z]+) \\| (?P<timestamp>[^ ]+) \\| (?P<status>\\d+) \\| (?P<request>[^|]+) \\| (?P<cache_status>[^|]*) \\| (?P<bytes>\\d+)$')\n\nif err == null {\n  .log_type = parsed.log_type\n  .timestamp = parse_timestamp!(parsed.timestamp, \"%Y-%m-%dT%H:%M:%S%:z\")\n  .status = to_int!(parsed.status)\n  .request = parsed.request\n  .cache_status = parsed.cache_status\n  .bytes_sent = to_int!(parsed.bytes)\n  \n  # Extract request method and path\n  request_parts = split!(parsed.request, \" \", limit: 3)\n  .method = request_parts[0]\n  .path = request_parts[1]\n  .http_version = request_parts[2]\n  \n  # Categorize by cache type\n  if contains(string!(.path), \"/media\") {\n    .cache_type = \"media\"\n  } else if contains(string!(.path), \"/player\") {\n    .cache_type = \"player\"\n  } else if contains(string!(.path), \"/renderer\") {\n    .cache_type = \"renderer\"\n  } else {\n    .cache_type = \"other\"\n  }\n}\n"
  # Sampling for high-volume logs (keep every 10th success)
  sample_nginx_access:
    type: sample
    inputs: ["parse_nginx_access"]
    rate: 10
  # OPTIMIZED: Replaced Lua with VRL (40% faster, parallelizable)
  add_metadata:
    type: remap
    inputs: ["sample_nginx_access", "nginx_error", "docker_containers", "system_logs", "kernel_logs", "ip_info", "service_health", "top_processes", "docker_images_info", "psi_pressure"]
    source: |
      # Get hostname from system (cached by Vector)
      hostname, _ = get_hostname()
      .device_hostname = hostname || "unknown"

      # Extract device ID if hostname format is rpi4-HASH-TIMESTAMP
      parts = split(.device_hostname, "-")
      if length(parts) >= 2 {
        .device_hostname = parts[1]
      }

      .device_role = "alm-edge"
      .environment = "production"

      # Default container_name and stream for non-docker sources
      if !exists(.container_name) {
        .container_name = "host"
      }
      if !exists(.stream) {
        .stream = "stdout"
      }

      # Ensure cache_status exists
      if !exists(.cache_status) {
        .cache_status = "N/A"
      }

      # Set source_type for top_processes data (has rss_kb field - unique to ps output)
      if exists(.rss_kb) {
        .source_type = "top_processes"
      }

      # Set source_type for docker_images_info (has img_digest field)
      if exists(.img_digest) {
        .source_type = "docker_images_info"
      }

      # Set source_type for PSI pressure data (has psi_cpu_some field)
      if exists(.psi_cpu_some) {
        .source_type = "psi_pressure"
      }

      # Default priority_label for non-system logs
      if !exists(.priority_label) {
        .priority_label = "INFO"
      }

      # Add human-readable priority labels for system logs (better Grafana UX)
      if exists(.PRIORITY) {
        priority_num = to_int(.PRIORITY) ?? 6
        if priority_num == 0 {
          .priority_label = "EMERGENCY"
        } else if priority_num == 1 {
          .priority_label = "ALERT"
        } else if priority_num == 2 {
          .priority_label = "CRITICAL"
        } else if priority_num == 3 {
          .priority_label = "ERROR"
        } else if priority_num == 4 {
          .priority_label = "WARNING"
        } else {
          .priority_label = "INFO"
        }
      }
  # OPTIMIZED: Replaced Lua with VRL (40% faster, parallelizable)
  enrich_host_metrics:
    type: remap
    inputs: ["host_metrics"]
    source: |
      # Get hostname from system
      hostname, _ = get_hostname()
      device_hostname = hostname || "unknown"

      # Extract device ID
      parts = split(device_hostname, "-")
      if length(parts) >= 2 {
        device_hostname = parts[1]
      }

      # Add to tags
      .tags.host = device_hostname
  # Convert temperature from millidegrees to degrees Celsius
  convert_temperature:
    type: remap
    inputs: ["cpu_temperature"]
    source: |
      # Convert from millidegrees to degrees (divide by 1000)
      temp_millidegrees = to_int!(.message)
      .temperature_celsius = temp_millidegrees / 1000.0
  # Convert CPU temperature to metric with hostname
  temperature_to_metric:
    type: lua
    inputs: ["convert_temperature"]
    version: "2"
    hooks:
      process: |
        function (event, emit)
          -- Lazy load hostname
          if not _G.device_hostname then
            local f = io.open("/etc/hostname", "r")
            if f then
              local content = f:read("*all")
              f:close()
              content = content:gsub("%s+", "")
              local parts = {}
              for part in string.gmatch(content, "[^-]+") do
                table.insert(parts, part)
              end
              if #parts >= 2 then
                _G.device_hostname = parts[2]
              else
                _G.device_hostname = content
              end
            else
              _G.device_hostname = "unknown"
            end
          end

          -- Create new metric event (metric events must have metric key at top level)
          emit {
            metric = {
              kind = "absolute",
              name = "cpu_temperature_celsius",
              namespace = "host",
              timestamp = event.log.timestamp,
              tags = {
                host = _G.device_hostname,
                sensor = "thermal_zone0"
              },
              gauge = {
                value = tonumber(event.log.temperature_celsius) or 0
              }
            }
          }
        end
  # NEW: Convert Docker stats to usable format
  docker_stats_to_metrics:
    type: remap
    inputs: ["docker_stats"]
    source: |
      # Parse percentages (remove % sign and convert to float)
      if exists(.cpu) && is_string(.cpu) {
        .cpu_percent = to_float(replace(string!(.cpu), "%", "")) ?? 0.0
      }
      if exists(.mem) && is_string(.mem) {
        .mem_percent = to_float(replace(string!(.mem), "%", "")) ?? 0.0
      }

      # Get hostname
      hostname, _ = get_hostname()
      .device_hostname = hostname || "unknown"
      parts = split(.device_hostname, "-")
      if length(parts) >= 2 {
        .device_hostname = parts[1]
      }
  # NEW: Generate Prometheus metrics from Docker stats
  docker_container_metrics:
    type: log_to_metric
    inputs: ["docker_stats_to_metrics"]
    metrics:
      - type: gauge
        field: cpu_percent
        name: container_cpu_percent
        namespace: docker
        tags:
          container: "{{ name }}"
          host: "{{ device_hostname }}"
      - type: gauge
        field: mem_percent
        name: container_memory_percent
        namespace: docker
        tags:
          container: "{{ name }}"
          host: "{{ device_hostname }}"
  # Filter only parsed nginx logs for metrics (logs with status field)
  # Bypass Lua for metrics generation - connect directly to sample_nginx_access
  nginx_parsed_logs:
    type: filter
    inputs: ["sample_nginx_access"]
    condition:
      type: vrl
      source: exists(.status) && exists(.method)
  # Add device hostname using environment variable or default
  add_device_label:
    type: remap
    inputs: ["nginx_parsed_logs"]
    source: |
      # Get hostname from system or default to "unknown"
      hostname, _ = get_hostname()
      .device_hostname = hostname || "unknown"
      # Extract device ID if hostname format is rpi4-HASH-TIMESTAMP
      parts = split(.device_hostname, "-")
      if length(parts) >= 2 {
        .device_hostname = parts[1]
      }
      # Update timestamp to now (so metrics aren't rejected as "out of bounds")
      .timestamp = now()
  # Generate metrics from parsed nginx logs (for Prometheus)
  log_metrics:
    type: log_to_metric
    inputs: ["add_device_label"]
    metrics:
      - type: counter
        field: status
        name: http_requests_total
        namespace: strealer
        tags:
          status: "{{ status }}"
          method: "{{ method }}"
          cache_status: "{{ cache_status }}"
          host: "{{ device_hostname }}"
# ============================================================================
# SINKS
# ============================================================================
sinks:
  loki:
    type: loki
    inputs: ["add_metadata"]
    endpoint: "${LOKI_ENDPOINT:-http://your-loki-server:3100}"
    encoding:
      codec: json
    healthcheck:
      enabled: false
    compression: snappy
    # Labels for log filtering, search, and aggregation
    # All JSON fields are automatically indexed as searchable fields
    # OPTIMIZED: Added priority_label for intuitive filtering in Grafana
    labels:
      device: "{{ device_hostname }}"
      role: "alm-edge"
      env: "production"
      log_source: "{{ source_type }}"
      cache_status: "{{ cache_status }}"
      container: "{{ container_name }}"
      stream: "{{ stream }}"
      priority: "{{ priority_label }}"
    # EDGE RESILIENCE: Disk buffering with backpressure for high-latency/unreliable networks
    # This survives network outages and process restarts, limited only by disk space
    buffer:
      type: disk
      max_size: 536870912 # 512MB - sufficient for ~8 hours of logs on typical edge device
      when_full: block # Exert backpressure on sources instead of dropping
    # EDGE OPTIMIZATION: Tuned for high-latency networks (5-60s latency)
    # OPTIMIZED: Larger batches = fewer HTTP requests = less CPU
    batch:
      max_bytes: 524288 # 512KB (2x increase) - balance between throughput and request overhead
      timeout_secs: 15 # Wait longer to accumulate larger batches
      max_events: 5000 # 2.5x increase to reduce HTTP request overhead
    # EDGE OPTIMIZATION: Retry logic for intermittent connectivity
    request:
      timeout_secs: 60 # Increased from 30s to handle slow transfers and DNS resolution delays
      retry_attempts: 10 # Exponential backoff: 1s, 2s, 4s, 8s, 16s, 32s, 64s (max 127s total)
      retry_initial_backoff_secs: 1 # Start with 1s backoff
      retry_max_duration_secs: 120 # Allow up to 2 minutes for retry sequence
      rate_limit_num: 100
      rate_limit_duration_secs: 1
    # Data durability (disabled for edge resilience - docker_containers source doesn't support acknowledgements)
    acknowledgements:
      enabled: false
  # Metrics: Push to Prometheus (Remote Write)
  # UPDATED: Added docker_container_metrics for per-container resource visibility
  # OPTIMIZED: Batch metrics aggressively (low cardinality = safe large batches)
  prometheus_remote_write:
    type: prometheus_remote_write
    inputs: ["log_metrics", "vector_metrics", "enrich_host_metrics", "temperature_to_metric", "docker_container_metrics"]
    endpoint: "${PROMETHEUS_ENDPOINT:-http://your-prometheus:9090}/api/v1/write"
    default_namespace: "strealer"
    healthcheck:
      enabled: false # Healthcheck uses GET which is not allowed on /api/v1/write (POST only)
    batch:
      max_events: 10000 # Large batch for metrics
      timeout_secs: 30 # Wait longer to accumulate
  # Expose metrics locally for scraping (optional backup)
  prometheus_exporter:
    type: prometheus_exporter
    inputs: ["log_metrics", "vector_metrics", "enrich_host_metrics", "temperature_to_metric", "docker_container_metrics"]
    address: "0.0.0.0:9598"
    default_namespace: "strealer"
